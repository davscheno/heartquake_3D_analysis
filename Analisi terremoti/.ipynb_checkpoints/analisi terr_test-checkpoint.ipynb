{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANTE ATTENZIONE OCCORRE CONVERTIRE IL FILE DIINGRESSO DEI PUNTI\n",
    "# IN COORDINATE PIANE METRICHE \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import mplstereonet\n",
    "from geopandas import GeoDataFrame\n",
    "from shapely.geometry import Point\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Import plotly package\n",
    "import plotly\n",
    "#plotly.tools.set_credentials_file(username='davide.schenone', api_key='WBSgVI0FXgSIRecK2cHU')\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "#from mpl_toolkits.basemap import pyproj\n",
    "#from mpl_toolkits.basemap import Basemap\n",
    "from pyproj import Proj, transform\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install geopandas\n",
    "#!pip install mplstereonet\n",
    "#!pip install scipy\n",
    "#!pip install sklearn\n",
    "#!pip install chart_studio\n",
    "#!pip install xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob(\"*.txt\")\n",
    "with open('Terremoti.csv', 'w') as outfile:\n",
    "    for fname in filenames:\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/Tesi/lib/python3.7/site-packages/pyproj/crs/crs.py:53: FutureWarning:\n",
      "\n",
      "'+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "\n",
      "/opt/anaconda3/envs/Tesi/lib/python3.7/site-packages/pyproj/crs/crs.py:294: FutureWarning:\n",
      "\n",
      "'+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "\n",
      "/opt/anaconda3/envs/Tesi/lib/python3.7/site-packages/pyproj/crs/crs.py:53: FutureWarning:\n",
      "\n",
      "'+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "\n",
      "/opt/anaconda3/envs/Tesi/lib/python3.7/site-packages/pyproj/crs/crs.py:294: FutureWarning:\n",
      "\n",
      "'+init=<authority>:<code>' syntax is deprecated. '<authority>:<code>' is the preferred initialization method. When making the change, be mindful of axis order changes: https://pyproj4.github.io/pyproj/stable/gotchas.html#axis-order-changes-in-proj-6\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can only use .dt accessor with datetimelike values",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-7f6a55ffbce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Deltatime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data_2'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1989\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m#df['Deltatime'] = df['Deltatime'].astype(datetime.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Deltatime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Data_2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m#con questo posso fare dei cluster temporali\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/Tesi/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5268\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5269\u001b[0m         ):\n\u001b[0;32m-> 5270\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5271\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/Tesi/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# http://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/Tesi/lib/python3.7/site-packages/pandas/core/indexes/accessors.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(cls, data)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mDatetimeProperties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .dt accessor with datetimelike values\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .dt accessor with datetimelike values"
     ]
    }
   ],
   "source": [
    "# Check ploltly version\n",
    "plotly.__version__\n",
    "\n",
    "# To communicate with Plotly's server, sign in with credentials file\n",
    "import chart_studio.plotly as py\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "\n",
    "# Create data with x and y random over [-2, 2], and z a Gaussian function of x and y.\n",
    "#np.random.seed(12345)\n",
    "#x = 2 * (np.random.random(500) - 0.5)\n",
    "#y = 2 * (np.random.random(500) - 0.5)\n",
    "\n",
    "#def f(x, y):\n",
    " #   return np.exp(-(x + y ** 2))\n",
    "\n",
    "#z = f(x, y)\n",
    "\n",
    "#imposto i dati\n",
    "#importa il file originale\n",
    "#df = pd.read_csv('Saorge_Taggia_erh_erz_prof.csv', sep='\\t', names=['y', 'x','z','a','b'])\n",
    "\n",
    "#importa il file convertito con convergo\n",
    "#df = pd.read_excel('dati_terremoti/dati_terremoti.xlsx', sheetname='Foglio1')\n",
    "\n",
    "df = pd.read_excel('terremoti_santo.xlsx', sheet_name='Foglio1')\n",
    "df['Data'] = df['Data'].astype(str).str.zfill(6)\n",
    "\n",
    "#converto le coordinate wgs 84 in UTM epsg 3003 Gauss-Boaga \n",
    "x1 = np.array(df['Longit.'])\n",
    "y1 = np.array(df['Latitud.'])\n",
    "\n",
    "\n",
    "inProj = Proj(init='epsg:4326')\n",
    "outProj = Proj(init='epsg:3003')\n",
    "x2,y2 = transform(inProj,outProj, x1, y1)\n",
    "\n",
    "df['x'] = x2\n",
    "df['y'] = y2\n",
    "\n",
    "df['z'] = df['Prof corr']  #creo il campo z profondità con valori negativi\n",
    "\n",
    "\n",
    "df['Data'] = df['Data'].astype(str).str.zfill(6) #riempie la mancanza dello zero nei primi valori\n",
    "df['Data'] =  pd.to_datetime(df['Data'],  format='%y%m%d')\n",
    "#df['Ora'] =  pd.to_time(df['Ora'],  format='%H%M')\n",
    "#creo una colonna con  la differenza in giorni tra la data e una posta abbastanza indietro nel passato\n",
    "df['Data_2'] = pd.to_datetime(df['Data']).dt.date\n",
    "df['Deltatime'] = df['Data_2'] - datetime.date(1989,1,1)\n",
    "#df['Deltatime'] = df['Deltatime'].astype(datetime.timedelta).map(lambda x: np.nan if pd.isnull(x) else x.days)\n",
    "df['Deltatime'] = df['Data_2'].dt.days\n",
    "#con questo posso fare dei cluster temporali\n",
    "\n",
    "#IMPOSTO I FILTRI\n",
    "\n",
    "#df_filter = df.query('Erh<1 and Erz<4') #imposto i parametri di filtro sugli errori orizzontali e verticali\n",
    "\n",
    "\n",
    "#df_filter = df.query('<Deltatime<') #imposto il filtro sulla data\n",
    "\n",
    "#imposto i pesi per il calcolo del piano da utilizzare in caso di regressione lineare pesata\n",
    "\n",
    "df['weight_calc'] = df['Nf']/24 - 0.25\n",
    "def weight(df):\n",
    "    if df['Nf'] <=6 :\n",
    "        return 0\n",
    "    \n",
    "    elif  df['Nf'] > 6 and df['Nf']  < 30   :\n",
    "        return df['weight_calc']\n",
    "            \n",
    "    elif  df['Nf'] >= 30 :\n",
    "        return 1\n",
    "\n",
    "df['weights'] =  df.apply(weight, axis=1) \n",
    " \n",
    "\n",
    "    \n",
    "    \n",
    "df_filter = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedo con il PRIMO  clustering attraverso DBSCAN (BASE TEMPORALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-f54aa5ce38ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcoords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_filter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Deltatime'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_filter' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "coords = df_filter.as_matrix(columns=['Deltatime'])\n",
    "\n",
    "\n",
    "\n",
    "db = DBSCAN(eps=70, min_samples=10).fit(coords)\n",
    "labels = db.labels_\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "df_filter = df_filter.assign(Classe = labels)\n",
    "\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inserisco il valore K su cui far girare il ciclo FOR\n",
    "cicli = df_filter['Classe'].value_counts().index.tolist()\n",
    "cicli = np.array(cicli)\n",
    "cicli_clean = np.delete(cicli, np.where(cicli == [-1]), axis=0)# cancello il label -1 che rappresente gli outliers\n",
    "plot_dip_direction = 0\n",
    "plot_dip = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CICLO FOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for K in cicli_clean:\n",
    "    \n",
    "    \n",
    "    #filtro la label di interesse\n",
    "    df_cluster = df_filter[df_filter['Classe'] == K]\n",
    "    df_filter_2 = df_cluster\n",
    "    coords_2 = df_cluster.as_matrix(columns=['x','y','z'])\n",
    "\n",
    "    \n",
    "    \n",
    "    #secondo filtraggio con DBSCAN\n",
    "    db = DBSCAN(eps=3000, min_samples=5).fit(coords_2)\n",
    "    labels_2 = db.labels_\n",
    "    from collections import Counter\n",
    "\n",
    "\n",
    "    df_filter_2 = df_filter_2.assign(Classe_spaziale = labels_2)\n",
    "    \n",
    "    #inserisco il valore J su cui far girare il SECONDO ciclo FOR\n",
    "    cicli_2 = df_filter_2['Classe_spaziale'].value_counts().index.tolist()\n",
    "    cicli_2 = np.array(cicli_2)\n",
    "    cicli_2_clean = np.delete(cicli_2, np.where(cicli_2 == [-1]), axis=0)# cancello il label -1 che rappresente gli outliers\n",
    "\n",
    "\n",
    " ##################### SECONDO CICLO ######################################   \n",
    "    \n",
    "    \n",
    "    for J in cicli_2_clean:\n",
    "    \n",
    "    \n",
    "        #filtro la label di interesse\n",
    "        df_cluster_2 = df_filter_2[df_filter_2['Classe_spaziale'] == J]\n",
    "        \n",
    "        \n",
    "        inizio = df_cluster_2.iloc[0,0] # setto la data inizio e fine del cluster\n",
    "        fine = df_cluster_2.iloc[-1,0]\n",
    "        \n",
    "        \n",
    "        #Creo una mappa dei punti su google maps\n",
    "\n",
    "        from gmplot import gmplot\n",
    "        gmap = gmplot.GoogleMapPlotter( 44.5514, 9.4335, 13)\n",
    "\n",
    "        # Scatter points\n",
    "        top_attraction_lats = df['Latitud.']\n",
    "        top_attraction_lons = df['Longit.']\n",
    "\n",
    "\n",
    "        filter_attraction_lats = df_filter['Latitud.']\n",
    "        filter_attraction_lons = df_filter['Longit.']\n",
    "\n",
    "        cluster_attraction_lats = df_cluster_2['Latitud.']# *******ATTENZIONE SONO DATI FILTRATI E CLUSTERIZZATI\n",
    "        cluster_attraction_lons = df_cluster_2['Longit.']\n",
    "\n",
    "\n",
    "\n",
    "        gmap.scatter(top_attraction_lats, top_attraction_lons, '#3B0B39', size=100, marker=False)\n",
    "\n",
    "        #gmap.scatter(filter_attraction_lats, filter_attraction_lons, '#f90707', size=40, marker=False)\n",
    "\n",
    "        gmap.scatter(cluster_attraction_lats, cluster_attraction_lons, '#1307f9', size=50, marker=False)\n",
    "\n",
    "        gmap.heatmap(cluster_attraction_lats, cluster_attraction_lons ) # crea una heatmap\n",
    "        # Draw\n",
    "        gmap.draw(\"my_map_time%f%f.html\"% (K,J))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        #importo il file depurato degli outlayer:\n",
    "        #df = pd.read_csv('taggia_saorge_ripulita_outlier.csv', sep=',', names=['x', 'y','z'])\n",
    "        \n",
    "        x = df_cluster_2['x']  \n",
    "    \n",
    "        y = df_cluster_2['y']   \n",
    "        \n",
    "        z = df_cluster_2['z']\n",
    "\n",
    "        W = df_cluster_2['weights']\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "        data = np.c_[x,y,z]\n",
    "\n",
    "        # regular grid covering the domain of the data\n",
    "        mn = np.min(data, axis=0)\n",
    "        mx = np.max(data, axis=0)\n",
    "        X,Y = np.meshgrid(np.linspace(mn[0], mx[0], 20), np.linspace(mn[1], mx[1], 20))\n",
    "        XX = X.flatten()\n",
    "        YY = Y.flatten()\n",
    "    \n",
    "        # best-fit linear plane (1st-order)\n",
    "        # applico i pesi\n",
    "\n",
    "        A = np.c_[data[:,0], data[:,1], np.ones(data.shape[0])]\n",
    "        L = data[:,2]\n",
    "        AW = A * np.sqrt(W[:,np.newaxis]) # intermedi con pesi per  calcolo dei coefficienti\n",
    "        LW = L * np.sqrt(W)               # intermedi con pesi per  calcolo dei coefficienti\n",
    "\n",
    "        C,_,_,_ = scipy.linalg.lstsq(AW, LW)    # coefficients\n",
    "\n",
    "        # evaluate it on grid\n",
    "        Z = C[0]*X + C[1]*Y + C[2]\n",
    "    \n",
    "        ####################################################################################################\n",
    "        #ricavo la direzione e l'immersione del piano attraverso l'equazione del piano e i coseni direttori\n",
    "\n",
    "        A=C[0]\n",
    "        B=C[1]\n",
    "        C_=-1\n",
    "        D=C[2]\n",
    "\n",
    "        #calcolo E\n",
    "        E=np.sqrt(A**2 + B**2 + 1)\n",
    "\n",
    "        #calcolo i coseni\n",
    "        cos_a = A/E\n",
    "\n",
    "        cos_b = B/E\n",
    "\n",
    "        cos_cp = C_/E\n",
    "\n",
    "\n",
    "        cos_c = np.cos((np.pi/2) + np.arccos(C_/E))   \n",
    "        \n",
    "    \n",
    "        #print(cos_a, cos_b, cos_c, cos_cp, cos_a_deg, cos_b_deg)\n",
    "\n",
    "       \n",
    "        #calcolo la direzione teta del vettore, il coseni direttori danno la direzione del vettore occorrre aggiungere 90°\n",
    "        teta_primo = np.arctan(cos_a/cos_b)   #espresso in radianti\n",
    "        teta_deg = (teta_primo*180)/np.pi \n",
    "\n",
    "        #calcolo della direzione della massima pendenza sul piano\n",
    "        #dip_direction = teta_deg \n",
    "\n",
    "        #calcolo l'immersione dip angolo tra la verticale e il piano\n",
    "        dip_vert = np.arcsin(-1*cos_c)\n",
    "        dip_deg = (dip_vert*180)/np.pi\n",
    "\n",
    "        #calcolo l'immersione dip\n",
    "        dip = dip_deg\n",
    "\n",
    "        #occorre orientare correttamente il dip_direction\n",
    "        if cos_a > 0 and cos_b > 0:                \n",
    "            dip_direction = teta_deg\n",
    "            if cos_cp <0:\n",
    "                dip_direction =  dip_direction + 180\n",
    "        \n",
    "        if  cos_a > 0 and cos_b < 0:\n",
    "            dip_direction = teta_deg + 180\n",
    "            if cos_cp <0:\n",
    "                dip_direction =  dip_direction + 180\n",
    "        \n",
    "            \n",
    "        if  cos_a < 0 and cos_b < 0:\n",
    "            dip_direction = teta_deg + 180\n",
    "            if cos_cp <0:\n",
    "                dip_direction =  dip_direction - 180\n",
    "       \n",
    "    \n",
    "        if  cos_a < 0 and cos_b > 0:\n",
    "            dip_direction =  teta_deg + 360\n",
    "            if cos_cp <0:\n",
    "                dip_direction =  dip_direction - 180\n",
    "                 \n",
    "        \n",
    "         #creo l'arrai per il plotaggio su stereogramma\n",
    "        plot_dip_direction = np.append(plot_dip_direction, dip_direction)\n",
    "        plot_dip = np.append(plot_dip, dip)\n",
    "        print('##################################################')\n",
    "        print(K,J)\n",
    "        print(\"dip_direction= %f ; dip= %f\" % (dip_direction, dip))\n",
    "        #print(cos_a, cos_b)\n",
    "     \n",
    "        output = \"teta= %f ; dip= %f\" % (dip_direction, dip)\n",
    "        file = open(\"giacitura_cluster_%f%f.txt\" %(K,J),\"w\")\n",
    "        file.write(output)\n",
    "        file.close()\n",
    "\n",
    "        daframe_cluster_xx = pd.DataFrame.to_csv(df_cluster_2)\n",
    "        file = open(\"daframe_cluster_%f%f.csv\" %(K,J),\"w\")\n",
    "        file.write(daframe_cluster_xx)\n",
    "        file.close()\n",
    "        print(\"Controllo di aver eliminato scatter outlayer [-1]\" , cicli_clean , cicli_2_clean) \n",
    "        \n",
    "        \n",
    "        #salva i risultati su di un file shape\n",
    "        geometry = [Point(xy) for xy in zip(df_cluster_2.x , df_cluster_2.y)]\n",
    "        df_shape = df_cluster_2.drop(['x', 'y','Ora', 'Data', 'Sec'], axis=1)\n",
    "        crs = {'init': 'epsg:3003'}\n",
    "        gdf = GeoDataFrame( df_shape, crs=crs, geometry=geometry)   #df_shape,\n",
    "        gdf.to_file(driver = 'ESRI Shapefile', filename= \"result%f%f.shp\" %(K,J))\n",
    "\n",
    "        \n",
    "        \n",
    "        #crea gli stereogrammi\n",
    "strike = plot_dip_direction - 90 \n",
    "dip =  plot_dip\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='stereonet')\n",
    "ax.plane(strike, dip)\n",
    "ax.pole(strike, dip ,markersize=5)\n",
    "ax.grid()\n",
    "plt.show()\n",
    "\n",
    "#density plot\n",
    "fig, ax = mplstereonet.subplots()\n",
    "cax = ax.density_contourf(strike, dip, measurement='poles')\n",
    "ax.pole(strike, dip)\n",
    "ax.grid(True)\n",
    "fig.colorbar(cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
